"""
Exploit analysis scrapers.

Per scraper.md requirements:
- Extract all text content from JS pages, especially markdown
- Download PDFs if available and store in separate folder
- Store raw HTML/JSON for traceability
"""
from __future__ import annotations

import re
from typing import Optional
from urllib.parse import urljoin, urlparse

from config.settings import EXPLOITS_DIR
from utils.helpers import ensure_dir
from utils.logger import log

from .base_scraper import BaseScraper


def _is_same_host(url: str, base_url: str) -> bool:
    """Check if URL belongs to the same host as base_url."""
    base_host = urlparse(base_url).netloc
    host = urlparse(url).netloc
    return not host or host == base_host


def _extract_article_links(soup, base_url: str, selectors: Optional[list[str]] = None) -> list[dict]:
    """Extract article links using multiple selector strategies."""
    items = []
    seen_urls = set()

    default_selectors = [
        "article h2 a[href]",
        "article h1 a[href]",
        "h2.entry-title a[href]",
        "h1.entry-title a[href]",
        ".post-title a[href]",
        ".article-title a[href]",
        "article a[href]",
    ]
    selectors = selectors or default_selectors

    for selector in selectors:
        for link in soup.select(selector):
            href = link.get("href", "").strip()
            if not href or href in seen_urls:
                continue
            if href.startswith(("#", "mailto:", "javascript:")):
                continue

            url = urljoin(f"{base_url}/", href)
            if not _is_same_host(url, base_url):
                continue

            title = link.get_text(strip=True) or link.get("aria-label") or link.get("title")
            if not title or len(title) < 3:
                continue

            seen_urls.add(href)
            items.append({"title": title, "url": url})

    return items


class RektNewsScraper(BaseScraper):
    """
    Scrape Rekt News incident reports.

    Rekt.news is a leading DeFi hack/exploit news site with detailed
    post-mortem reports on security incidents.
    """

    SOURCE = "rekt_news"
    ARTICLE_PATTERN = re.compile(r"^/[\w\-]+/$")

    def __init__(self, output_dir=None):
        output_dir = output_dir or ensure_dir(EXPLOITS_DIR / self.SOURCE)
        super().__init__(base_url="https://rekt.news", output_dir=output_dir, requires_js=False)

    def _get_article_links(self, soup) -> list[dict]:
        """Extract article links from Rekt News listing."""
        links = []
        seen = set()

        # Rekt news uses specific article structure
        for article in soup.select("article, .post, .entry"):
            link = article.find("a", href=True)
            if not link:
                continue

            href = link.get("href", "")
            if not href or href in seen:
                continue
            if href in ("/", "/leaderboard", "/about"):
                continue

            url = urljoin(f"{self.base_url}/", href)
            title = link.get_text(strip=True)

            # Also try to find title in heading
            heading = article.find(["h1", "h2", "h3"])
            if heading:
                title = heading.get_text(strip=True)

            if title and len(title) > 3 and url not in [l["url"] for l in links]:
                seen.add(href)
                links.append({"title": title, "url": url})

        # Fallback to generic extraction
        if not links:
            links = _extract_article_links(soup, self.base_url)

        return links

    def scrape(self) -> list[dict]:
        """Scrape all Rekt News incident reports with full content."""
        items: list[dict] = []

        # Main page with pagination
        start_url = self.base_url
        log.info(f"{self.SOURCE}: Starting pagination from {start_url}")

        page_urls = self.handle_pagination(start_url, max_pages=20)
        log.info(f"{self.SOURCE}: Found {len(page_urls)} pages")

        all_links = []
        for page_url in page_urls:
            try:
                html = self.fetch(page_url)
                self.save_raw_html(html, page_url, prefix="listing")
            except Exception as exc:
                log.warning(f"{self.SOURCE}: failed to fetch {page_url}: {exc}")
                continue

            soup = self.parse_html(html)
            links = self._get_article_links(soup)
            all_links.extend(links)

        # Dedupe links
        seen_urls = set()
        unique_links = []
        for link in all_links:
            if link["url"] not in seen_urls:
                seen_urls.add(link["url"])
                unique_links.append(link)

        log.info(f"{self.SOURCE}: Found {len(unique_links)} unique article links")

        # Scrape each article detail page
        for link_info in unique_links:
            detail = self.scrape_detail_page(link_info["url"])
            detail.update({
                "source": self.SOURCE,
                "category": "exploit",
                "listing_title": link_info["title"],
            })

            # Download any PDFs found
            for pdf_url in detail.get("pdf_links", []):
                pdf_path = self.download_pdf(pdf_url)
                if pdf_path:
                    detail.setdefault("downloaded_pdfs", []).append(str(pdf_path))

            items.append(detail)

        items = self.dedupe_items(items)
        self.save_report(self.build_payload(self.SOURCE, items), f"{self.SOURCE}_reports")
        log.info(f"{self.SOURCE}: Scraped {len(items)} incident reports")
        return items


class TrailOfBitsScraper(BaseScraper):
    """
    Scrape Trail of Bits blog posts for blockchain/security content.

    Trail of Bits is a leading security research firm with excellent
    technical blog posts on vulnerabilities and security research.
    """

    SOURCE = "trail_of_bits"
    ENDPOINTS = ["/category/blockchain/", "/category/security/"]

    def __init__(self, output_dir=None):
        output_dir = output_dir or ensure_dir(EXPLOITS_DIR / self.SOURCE)
        super().__init__(base_url="https://blog.trailofbits.com", output_dir=output_dir, requires_js=False)

    def _get_post_links(self, soup) -> list[dict]:
        """Extract blog post links from Trail of Bits blog."""
        links = []
        seen = set()

        # Trail of Bits blog structure
        for article in soup.select("article, .post, .entry, .hentry"):
            link = article.find("a", href=True)
            if not link:
                continue

            href = link.get("href", "")
            if not href or href in seen:
                continue
            if "/category/" in href or "/tag/" in href:
                continue

            url = urljoin(f"{self.base_url}/", href)
            title = link.get_text(strip=True)

            # Find title in heading
            heading = article.find(["h1", "h2", "h3"], class_=re.compile(r"entry-title|post-title"))
            if heading:
                title = heading.get_text(strip=True)

            if title and len(title) > 5 and url not in [l["url"] for l in links]:
                seen.add(href)
                links.append({"title": title, "url": url})

        # Fallback
        if not links:
            links = _extract_article_links(soup, self.base_url)

        return links

    def scrape(self) -> list[dict]:
        """Scrape Trail of Bits blog posts with full content."""
        items: list[dict] = []

        for endpoint in self.ENDPOINTS:
            start_url = self.build_url(endpoint)
            log.info(f"{self.SOURCE}: Starting pagination from {start_url}")

            page_urls = self.handle_pagination(start_url, max_pages=10)

            for page_url in page_urls:
                try:
                    html = self.fetch(page_url)
                    self.save_raw_html(html, page_url, prefix="listing")
                except Exception as exc:
                    log.warning(f"{self.SOURCE}: failed to fetch {page_url}: {exc}")
                    continue

                soup = self.parse_html(html)
                links = self._get_post_links(soup)

                for link_info in links:
                    detail = self.scrape_detail_page(link_info["url"])
                    detail.update({
                        "source": self.SOURCE,
                        "category": "exploit",
                        "listing_title": link_info["title"],
                        "blog_category": endpoint.strip("/").replace("category/", ""),
                    })

                    # Download any PDFs found
                    for pdf_url in detail.get("pdf_links", []):
                        pdf_path = self.download_pdf(pdf_url)
                        if pdf_path:
                            detail.setdefault("downloaded_pdfs", []).append(str(pdf_path))

                    items.append(detail)

        items = self.dedupe_items(items)
        self.save_report(self.build_payload(self.SOURCE, items), f"{self.SOURCE}_reports")
        log.info(f"{self.SOURCE}: Scraped {len(items)} blog posts")
        return items


class ImmunefiScraper(BaseScraper):
    """
    Scrape Immunefi bug bounty listings.

    Immunefi is the leading bug bounty platform for DeFi/Web3.
    Heavy JS rendering required.
    """

    SOURCE = "immunefi"
    ENDPOINTS = ["/bug-bounty/"]

    def __init__(self, output_dir=None):
        output_dir = output_dir or ensure_dir(EXPLOITS_DIR / self.SOURCE)
        super().__init__(base_url="https://immunefi.com", output_dir=output_dir, requires_js=True)

    def _get_bounty_links(self, soup) -> list[dict]:
        """Extract bug bounty program links from Immunefi."""
        links = []
        seen = set()

        for link in soup.select("a[href]"):
            href = link.get("href", "")
            if "/bug-bounty/" not in href:
                continue
            if href == "/bug-bounty/" or href == "/bug-bounty":
                continue

            url = urljoin(f"{self.base_url}/", href)
            if url in seen:
                continue
            seen.add(url)

            title = link.get_text(strip=True)
            if not title or len(title) < 2:
                # Try to get title from parent or sibling elements
                parent = link.parent
                if parent:
                    heading = parent.find(["h1", "h2", "h3", "h4"])
                    if heading:
                        title = heading.get_text(strip=True)

            if title and len(title) > 2:
                links.append({"title": title, "url": url})

        return links

    def scrape(self) -> list[dict]:
        """Scrape Immunefi bug bounty listings with full content."""
        items: list[dict] = []

        for endpoint in self.ENDPOINTS:
            url = self.build_url(endpoint)
            log.info(f"{self.SOURCE}: Fetching {url}")

            try:
                html = self.fetch(url)
                self.save_raw_html(html, url, prefix="listing")
            except Exception as exc:
                log.warning(f"{self.SOURCE}: failed to fetch {url}: {exc}")
                continue

            soup = self.parse_html(html)
            links = self._get_bounty_links(soup)
            log.info(f"{self.SOURCE}: Found {len(links)} bounty program links")

            for link_info in links:
                detail = self.scrape_detail_page(link_info["url"])
                detail.update({
                    "source": self.SOURCE,
                    "category": "exploit",
                    "listing_title": link_info["title"],
                    "bounty_type": "bug_bounty",
                })

                # Download any PDFs found (scope documents, etc.)
                for pdf_url in detail.get("pdf_links", []):
                    pdf_path = self.download_pdf(pdf_url)
                    if pdf_path:
                        detail.setdefault("downloaded_pdfs", []).append(str(pdf_path))

                items.append(detail)

        items = self.dedupe_items(items)
        self.save_report(self.build_payload(self.SOURCE, items), f"{self.SOURCE}_reports")
        log.info(f"{self.SOURCE}: Scraped {len(items)} bug bounty programs")
        return items


# Convenience function to run all exploit scrapers
def scrape_all_exploits() -> dict[str, list[dict]]:
    """Run all exploit/security scrapers and return combined results."""
    scrapers = [
        RektNewsScraper(),
        TrailOfBitsScraper(),
        ImmunefiScraper(),
    ]

    results = {}
    for scraper in scrapers:
        try:
            log.info(f"Running {scraper.SOURCE} scraper...")
            results[scraper.SOURCE] = scraper.scrape()
        except Exception as exc:
            log.error(f"Failed to run {scraper.SOURCE} scraper: {exc}")
            results[scraper.SOURCE] = []

    return results
