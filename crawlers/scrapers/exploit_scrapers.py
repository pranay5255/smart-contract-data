"""
Exploit analysis scrapers.
"""
from __future__ import annotations

from typing import Optional
from urllib.parse import urljoin, urlparse

from config.settings import EXPLOITS_DIR
from utils.helpers import ensure_dir
from utils.logger import log

from .base_scraper import BaseScraper


def _is_same_host(url: str, base_url: str) -> bool:
    base_host = urlparse(base_url).netloc
    host = urlparse(url).netloc
    return not host or host == base_host


def _extract_article_links(soup, base_url: str) -> list[dict]:
    items = []
    selectors = [
        "article h2 a[href]",
        "article h1 a[href]",
        "h2.entry-title a[href]",
        "h1.entry-title a[href]",
    ]
    seen = set()
    for selector in selectors:
        for link in soup.select(selector):
            href = link.get("href", "").strip()
            if not href or href in seen:
                continue
            url = urljoin(f"{base_url}/", href)
            if not _is_same_host(url, base_url):
                continue
            title = link.get_text(strip=True) or link.get("aria-label") or link.get("title")
            if not title:
                continue
            seen.add(href)
            items.append({"title": title, "url": url})
    return items


def _extract_links(soup, base_url: str, include_substrings: Optional[list[str]] = None) -> list[dict]:
    items = []
    for link in soup.select("a[href]"):
        href = link.get("href", "").strip()
        if not href or href.startswith(("#", "mailto:", "javascript:")):
            continue
        href_lower = href.lower()
        if include_substrings and not any(sub in href_lower for sub in include_substrings):
            continue
        url = urljoin(f"{base_url}/", href)
        if not _is_same_host(url, base_url):
            continue
        title = link.get_text(strip=True) or link.get("aria-label") or link.get("title")
        if not title:
            continue
        items.append({"title": title, "url": url})
    return items


class RektNewsScraper(BaseScraper):
    """Scrape Rekt News incident reports."""

    SOURCE = "rekt_news"
    ENDPOINTS = ["/"]

    def __init__(self, output_dir=None):
        output_dir = output_dir or ensure_dir(EXPLOITS_DIR / self.SOURCE)
        super().__init__(base_url="https://rekt.news", output_dir=output_dir, requires_js=False)

    def scrape(self) -> list[dict]:
        items: list[dict] = []
        for endpoint in self.ENDPOINTS:
            start_url = self.build_url(endpoint)
            page_urls = self.handle_pagination(start_url, max_pages=50)
            for page_url in page_urls:
                try:
                    html = self.fetch(page_url)
                except Exception as exc:
                    log.warning(f"{self.SOURCE}: failed to fetch {page_url}: {exc}")
                    continue
                soup = self.parse_html(html)
                links = _extract_article_links(soup, self.base_url)
                for link in links:
                    link.update({"source": self.SOURCE, "category": "exploit", "page": page_url})
                items.extend(links)

        items = self.dedupe_items(items)
        self.save_report(self.build_payload(self.SOURCE, items), f"{self.SOURCE}_reports")
        return items


class TrailOfBitsScraper(BaseScraper):
    """Scrape Trail of Bits blog posts for blockchain/security."""

    SOURCE = "trail_of_bits"
    ENDPOINTS = ["/category/blockchain", "/category/security"]

    def __init__(self, output_dir=None):
        output_dir = output_dir or ensure_dir(EXPLOITS_DIR / self.SOURCE)
        super().__init__(base_url="https://blog.trailofbits.com", output_dir=output_dir, requires_js=False)

    def scrape(self) -> list[dict]:
        items: list[dict] = []
        for endpoint in self.ENDPOINTS:
            start_url = self.build_url(endpoint)
            page_urls = self.handle_pagination(start_url, max_pages=50)
            for page_url in page_urls:
                try:
                    html = self.fetch(page_url)
                except Exception as exc:
                    log.warning(f"{self.SOURCE}: failed to fetch {page_url}: {exc}")
                    continue
                soup = self.parse_html(html)
                links = _extract_article_links(soup, self.base_url)
                for link in links:
                    link.update({"source": self.SOURCE, "category": "exploit", "page": page_url})
                items.extend(links)

        items = self.dedupe_items(items)
        self.save_report(self.build_payload(self.SOURCE, items), f"{self.SOURCE}_reports")
        return items


class ImmunefiScraper(BaseScraper):
    """Scrape Immunefi bug bounty listings."""

    SOURCE = "immunefi"
    ENDPOINTS = ["/bug-bounty"]

    def __init__(self, output_dir=None):
        output_dir = output_dir or ensure_dir(EXPLOITS_DIR / self.SOURCE)
        super().__init__(base_url="https://immunefi.com", output_dir=output_dir, requires_js=True)

    def scrape(self) -> list[dict]:
        items: list[dict] = []
        include_terms = ["/bug-bounty/"]
        for endpoint in self.ENDPOINTS:
            url = self.build_url(endpoint)
            try:
                html = self.fetch(url)
            except Exception as exc:
                log.warning(f"{self.SOURCE}: failed to fetch {url}: {exc}")
                continue
            soup = self.parse_html(html)
            links = _extract_links(soup, self.base_url, include_substrings=include_terms)
            for link in links:
                link.update({"source": self.SOURCE, "category": "exploit", "endpoint": endpoint})
            items.extend(links)

        items = self.dedupe_items(items)
        self.save_report(self.build_payload(self.SOURCE, items), f"{self.SOURCE}_reports")
        return items
